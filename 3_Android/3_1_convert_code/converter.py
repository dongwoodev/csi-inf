# -*- coding: utf-8 -*-
"""Converter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QwPcEkEfR0wZ61ue20o_1NRgdSU_vEe6
"""

# !pip install onnx
# !pip uninstall tensorflow onnx onnx-tf -y
# !pip install tensorflow==2.15.0
# !pip install onnx-tf==1.9.0
# !pip install onnxruntime

"""## Pytorch 모델 로드"""

import torch
import torch.onnx
import pandas as pd
import numpy as np
from CNN import CNN

model_path = "./model/original/act" # <<<<<<<<<<<<<- 모델 경로 입력
file_name = model_path.split("/")[-1]
model = CNN(n_classes= 3 if file_name == "act" else 2)
model.load_state_dict(torch.load(model_path, weights_only=False))
model.eval()


# .pt 형식으로 저장
torch_model_path = f"./model/torch/{file_name}.pt" 
torch.save(model.state_dict(), torch_model_path) 
print("PyTorch 모델이 .pt 파일로 저장되었습니다.")

"""## Pytorch 모델 저장"""

data = pd.read_csv("./AP_SIT_output.csv", header=None) # <<<<<<<<<<<<<- 데이터 경로 입력
data_numpy = data.values.astype('float32') # Numpy array
data_tensor = torch.tensor(data_numpy) # Tensor array
dummy_input = data_tensor.unsqueeze(0).unsqueeze(0) # 배치 및 채널 차원 추가
print("변환된 텐서 크기:", dummy_input.shape)
# dummy_input = dummy_input.transpose(2, 3)
print("변환된 텐서 크기:", dummy_input.shape)


torch_output = model(dummy_input).detach().numpy()
print("▶️ 기존 모델 출력: ", torch_output)

model.load_state_dict(torch.load(f"./model/torch/{file_name}.pt", weights_only=False))
model.eval()
torch_output = model(dummy_input).detach().numpy()
print("▶️▶️ PyTorch(.pt) 모델 출력:", torch_output)

"""# ONNX 모델 저장"""

from onnxruntime import InferenceSession
import pandas as pd

onnx_model_path = f"./model/onnx/{file_name}.onnx"
torch.onnx.export(model, dummy_input, onnx_model_path, export_params=True, opset_version=10, do_constant_folding=True, input_names=['input'], output_names=['output'])
print("PyTorch 모델이 ONNX 형식으로 변환되었습니다.")

onnx_session = InferenceSession(onnx_model_path)
onnx_input = {onnx_session.get_inputs()[0].name: dummy_input.numpy()}
onnx_output = onnx_session.run(None, onnx_input)
print("▶️▶️▶️ ONNX 모델 출력:", onnx_output[0])

"""# Tensorflow 모델 저장"""

import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load(onnx_model_path)
tf_rep = prepare(onnx_model)
tf_model_path = f"./model/tf/{file_name}.pb"
tf_rep.export_graph(tf_model_path)
print("ONNX 모델이 TensorFlow 형식으로 변환되었습니다.")

"""# Tensorflow-lite 모델 저장"""

import tensorflow as tf

# TensorFlow 모델 불러오기
converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_path)
tflite_model = converter.convert()
tflite_model_path = f"./model/tflite/{file_name}.tflite"
with open(tflite_model_path, "wb") as f:
    f.write(tflite_model)
print("TensorFlow 모델이 TensorFlow Lite 형식으로 변환되었습니다.")



# TensorFlow Lite 모델 로드
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# 입력 및 출력 텐서 정보 확인
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# 입력 데이터 설정
interpreter.set_tensor(input_details[0]['index'], dummy_input)

# 모델 실행
interpreter.invoke()

# 출력 데이터 확인
tflite_output = interpreter.get_tensor(output_details[0]['index'])
print("▶️▶️▶️▶️ TensorFlow Lite 모델 출력:", tflite_output)

# ------------------------------
logits = np.array(tflite_output)
softmax = np.exp(logits) / np.sum(np.exp(logits))
print("▶️▶️▶️▶️▶️ Softmax probabilities:", softmax)
# ------------------------------

